{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Subtask 1\n",
        "# X-Means and K-Medoids Clustering\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Requirements:\n",
        "\n",
        "- Python 3.13.1\n",
        "- numpy\n",
        "- pandas\n",
        "- scikit-learn\n",
        "- matplotlib\n",
        "- pyclustering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "from typing import List, Optional, Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas.api.types import is_integer_dtype\n",
        "from sklearn.calibration import LabelEncoder\n",
        "from sklearn.discriminant_analysis import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score, silhouette_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Pyclustering imports\n",
        "from pyclustering.cluster.xmeans import xmeans\n",
        "from pyclustering.cluster.kmedoids import kmedoids\n",
        "from pyclustering.utils.metric import distance_metric, type_metric\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing\n",
        "Load the data and preprocess it. Drop features we dont need. Since we are working with mixed-type data, we need to encode features that are not continuous numeric. Also, missing values need to be imputated.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"../dataset/tracks.csv\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Drop Irrelevant Features\n",
        "Many columns such as names or unique identifiers are not relevant for clustering. We drop them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "drop_cols = [\n",
        "    \"id\",\n",
        "    \"id_artist\",\n",
        "    \"name_artist\",\n",
        "    \"full_title\",\n",
        "    \"title\",\n",
        "    \"swear_IT_words\",\n",
        "    \"swear_EN_words\",\n",
        "    \"album_name\",\n",
        "    \"album_release_date\",\n",
        "    \"id_album\",\n",
        "    \"album_image\",\n",
        "    \"lyrics\",\n",
        "    \"month\",\n",
        "    \"day\",\n",
        "    \"disc_number\",\n",
        "    \"track_number\",\n",
        "    \"album_type\",\n",
        "    \"popularity\",\n",
        "    \"year\",\n",
        "    \"primary_artist\",\n",
        "    \"featured_artists\",\n",
        "    \"album\",\n",
        "]\n",
        "\n",
        "df = df.drop(columns=drop_cols)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Relevant preprocessing functions\n",
        "\n",
        "def clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    # Replace empty cells (or ?) with NA\n",
        "    df = df.replace(r\"^\\?|\\s+$\", pd.NA, regex=True)\n",
        "\n",
        "    # Columns with whitespace (or ?) values are infered as string type but could be numeric\n",
        "    for col in df.select_dtypes(exclude=[np.number, np.bool_]).columns:\n",
        "        # Exclude booleans\n",
        "        if df[col].nunique() > 2:\n",
        "            try:\n",
        "                df[col] = pd.to_numeric(df[col])\n",
        "            except ValueError:\n",
        "                logging.debug(\"Could not convert column %s to numeric dtype\", col)\n",
        "\n",
        "    # Integer columns with missing values automatically get converted to float columns\n",
        "    # We want to convert back to integer to allow for better column classification (num/cat)\n",
        "    for col in df.select_dtypes(include=np.floating):\n",
        "        if np.all(df[col].fillna(0) % 1 == 0):\n",
        "            df[col] = df[col].astype(\"Int64\")\n",
        "\n",
        "    # Turn object columns into string columns\n",
        "    for col in df.select_dtypes(include=object).columns:\n",
        "        df[col] = df[col].astype(str)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def scale_cols(\n",
        "    df: pd.DataFrame,\n",
        "    num_cols: Optional[List[str]],\n",
        "    cat_cols: Optional[List[str]],\n",
        "    bool_cols: Optional[List[str]] = None,\n",
        ") -> pd.DataFrame:\n",
        "    df = df.copy(deep=True)\n",
        "    if num_cols:\n",
        "        df[num_cols] = StandardScaler().fit_transform(df[num_cols])\n",
        "    if cat_cols:\n",
        "        df[cat_cols] = df[cat_cols].apply(LabelEncoder().fit_transform)\n",
        "    if bool_cols:\n",
        "        df[bool_cols] = df[bool_cols].apply(LabelEncoder().fit_transform)\n",
        "    return df\n",
        "\n",
        "\n",
        "def imputate_na(\n",
        "    df: pd.DataFrame,\n",
        "    num_cols: Optional[List[str]],\n",
        "    cat_cols: Optional[List[str]],\n",
        "    bool_cols: Optional[List[str]] = None,\n",
        ") -> pd.DataFrame:\n",
        "    if num_cols:\n",
        "        # imputate numeric features\n",
        "        for col in num_cols:\n",
        "            # For integer columns we need to round the median\n",
        "            if is_integer_dtype(df[col]):\n",
        "                df[col] = df[col].fillna(round(df[col].median()))\n",
        "            else:\n",
        "                df[col] = df[col].fillna(df[col].median())\n",
        "    if cat_cols is None:\n",
        "        cat_cols = []\n",
        "    if bool_cols is None:\n",
        "        bool_cols = []\n",
        "    # add \"missing\" category for categorical features\n",
        "    for col in cat_cols + bool_cols:\n",
        "        df[col] = df[col].astype(str)\n",
        "        df[col] = df[col].fillna(\"<NA>\")\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cleaning, Imputating, Scaling and Labelling\n",
        "Now execute the functions from above to return a dataframe we can use for computation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save old dataframe for later\n",
        "og_df = df.copy(deep=True)\n",
        "\n",
        "cat_cols = [\n",
        "    \"language\",\n",
        "]\n",
        "bool_cols = [\"explicit\"]\n",
        "num_cols = [\n",
        "    \"stats_pageviews\",\n",
        "    \"n_sentences\",\n",
        "    \"n_tokens\",\n",
        "    \"swear_IT\",\n",
        "    \"swear_EN\",\n",
        "    \"tokens_per_sent\",\n",
        "    \"char_per_tok\",\n",
        "    \"lexical_density\",\n",
        "    \"avg_token_per_clause\",\n",
        "    \"bpm\",\n",
        "    \"centroid\",\n",
        "    \"rolloff\",\n",
        "    \"flux\",\n",
        "    \"rms\",\n",
        "    \"zcr\",\n",
        "    \"flatness\",\n",
        "    \"spectral_complexity\",\n",
        "    \"pitch\",\n",
        "    \"loudness\",\n",
        "    \"duration_ms\",\n",
        "    \"modified_popularity\",\n",
        "]\n",
        "\n",
        "# Check we registered all columns\n",
        "assert set(cat_cols + bool_cols + num_cols) == set(df.columns)\n",
        "\n",
        "df = clean(df)\n",
        "df = imputate_na(df, num_cols, cat_cols, bool_cols)\n",
        "df = scale_cols(df, num_cols, cat_cols, bool_cols)\n",
        "\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encoding\n",
        "Now we use one-hot encoding to encode the categorical features. Boolean features only need encoding if they contain NaN or NA values (=missing values), otherwise they are left as is. To reduce the number of features, we only encode features that have more unique values than a threshhold. This will get rid of features with too many unique values, such as song lyrics. These features would add up to N new features to the dataset, which is not a good idea.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def one_hot_encode_feature(df: pd.DataFrame, feature_to_encode: str, weight: float = 1) -> pd.DataFrame:\n",
        "    dummies = pd.get_dummies(df[feature_to_encode], dtype=\"int32\", prefix=feature_to_encode)\n",
        "    dummies *= weight\n",
        "    result_df = pd.concat([df, dummies], axis=1)\n",
        "    return result_df.drop(columns=feature_to_encode)\n",
        "\n",
        "\n",
        "encoded_df = df.copy(deep=True)\n",
        "\n",
        "for col in cat_cols + [col for col in bool_cols if encoded_df[col].nunique() > 2]:\n",
        "    # Dont encode columns with too many unique values\n",
        "    if encoded_df[col].nunique() > 50:\n",
        "        logging.warning(\"Column %s has too many unique values, dropping\", col)\n",
        "        encoded_df = encoded_df.drop(columns=col)\n",
        "    else:\n",
        "        encoded_df = one_hot_encode_feature(encoded_df, col)\n",
        "\n",
        "encoded_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## X-Means Clustering\n",
        "X-means is an extension of k-means that automatically determines the optimal number of clusters. It starts with k=2 and uses statistical tests to decide whether to split clusters or stop. This eliminates the need to manually specify the number of clusters.\n",
        "\n",
        "### X-Means Algorithm\n",
        "X-means clustering is performed and evaluated using the same metrics as k-means:\n",
        "- **Sum of Squared Errors (SSE)**: Lower is better\n",
        "- **Silhouette Score**: Higher is better  \n",
        "- **Variance Ratio Criterion**: Higher is better\n",
        "- **Davies-Bouldin Index**: Lower is better\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert data to list format for pyclustering\n",
        "data = encoded_df.values.tolist()\n",
        "\n",
        "# X-means clustering\n",
        "xmeans_instance = xmeans(data, k_max=10)\n",
        "xmeans_instance.process()\n",
        "\n",
        "# Get results\n",
        "xmeans_centers = xmeans_instance.get_centers()\n",
        "xmeans_clusters = xmeans_instance.get_clusters()\n",
        "xmeans_labels = np.zeros(len(data), dtype=int)\n",
        "\n",
        "# Convert cluster format to labels\n",
        "for cluster_id, cluster in enumerate(xmeans_clusters):\n",
        "    for point_idx in cluster:\n",
        "        xmeans_labels[point_idx] = cluster_id\n",
        "\n",
        "print(f\"X-means found {len(xmeans_centers)} clusters\")\n",
        "print(f\"Cluster sizes: {[len(cluster) for cluster in xmeans_clusters]}\")\n",
        "\n",
        "# Calculate metrics for X-means\n",
        "xmeans_sse = 0\n",
        "for cluster_id, cluster in enumerate(xmeans_clusters):\n",
        "    center = xmeans_centers[cluster_id]\n",
        "    for point_idx in cluster:\n",
        "        point = data[point_idx]\n",
        "        xmeans_sse += sum((np.array(point) - np.array(center))**2)\n",
        "\n",
        "xmeans_silhouette = silhouette_score(encoded_df.values, xmeans_labels)\n",
        "xmeans_vrc = calinski_harabasz_score(encoded_df.values, xmeans_labels)\n",
        "xmeans_davies_bouldin = davies_bouldin_score(encoded_df.values, xmeans_labels)\n",
        "\n",
        "print(f\"X-means SSE: {xmeans_sse:.2f}\")\n",
        "print(f\"X-means Silhouette: {xmeans_silhouette:.3f}\")\n",
        "print(f\"X-means VRC: {xmeans_vrc:.2f}\")\n",
        "print(f\"X-means Davies-Bouldin: {xmeans_davies_bouldin:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## K-Medoids Clustering\n",
        "K-medoids is similar to k-means but uses actual data points (medoids) as cluster centers instead of centroids. This makes it more robust to outliers. We'll test different values of k and compare the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# K-medoids clustering for different k values\n",
        "kmedoids_metrics = {\n",
        "    'sse': [],\n",
        "    'silhouette': [],\n",
        "    'vrc': [],\n",
        "    'davies_bouldin': []\n",
        "}\n",
        "kmedoids_results = {}\n",
        "\n",
        "for k in range(2, 11):\n",
        "    # Initialize k-medoids with random medoids\n",
        "    initial_medoids = np.random.choice(len(data), k, replace=False).tolist()\n",
        "    \n",
        "    # Create k-medoids instance\n",
        "    kmedoids_instance = kmedoids(data, initial_medoids)\n",
        "    kmedoids_instance.process()\n",
        "    \n",
        "    # Get results\n",
        "    kmedoids_medoids = kmedoids_instance.get_medoids()\n",
        "    kmedoids_clusters = kmedoids_instance.get_clusters()\n",
        "    kmedoids_labels = np.zeros(len(data), dtype=int)\n",
        "    \n",
        "    # Convert cluster format to labels\n",
        "    for cluster_id, cluster in enumerate(kmedoids_clusters):\n",
        "        for point_idx in cluster:\n",
        "            kmedoids_labels[point_idx] = cluster_id\n",
        "    \n",
        "    kmedoids_results[k] = kmedoids_labels\n",
        "    \n",
        "    # Calculate SSE for k-medoids\n",
        "    kmedoids_sse = 0\n",
        "    for cluster_id, cluster in enumerate(kmedoids_clusters):\n",
        "        medoid_idx = kmedoids_medoids[cluster_id]\n",
        "        medoid = data[medoid_idx]\n",
        "        for point_idx in cluster:\n",
        "            point = data[point_idx]\n",
        "            kmedoids_sse += sum((np.array(point) - np.array(medoid))**2)\n",
        "    \n",
        "    kmedoids_metrics['sse'].append(kmedoids_sse)\n",
        "    kmedoids_metrics['silhouette'].append(silhouette_score(encoded_df.values, kmedoids_labels))\n",
        "    kmedoids_metrics['vrc'].append(calinski_harabasz_score(encoded_df.values, kmedoids_labels))\n",
        "    kmedoids_metrics['davies_bouldin'].append(davies_bouldin_score(encoded_df.values, kmedoids_labels))\n",
        "\n",
        "# Plot k-medoids metrics\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "k_range = range(2, 11)\n",
        "\n",
        "axes[0,0].plot(k_range, kmedoids_metrics['sse'], 'bo-')\n",
        "axes[0,0].set_title('K-Medoids SSE (Lower is better)')\n",
        "axes[0,0].set_xlabel('k')\n",
        "\n",
        "axes[0,1].plot(k_range, kmedoids_metrics['silhouette'], 'ro-')\n",
        "axes[0,1].set_title('K-Medoids Silhouette Score (Higher is better)')\n",
        "axes[0,1].set_xlabel('k')\n",
        "\n",
        "axes[1,0].plot(k_range, kmedoids_metrics['vrc'], 'go-')\n",
        "axes[1,0].set_title('K-Medoids Variance Ratio Criterion (Higher is better)')\n",
        "axes[1,0].set_xlabel('k')\n",
        "\n",
        "axes[1,1].plot(k_range, kmedoids_metrics['davies_bouldin'], 'mo-')\n",
        "axes[1,1].set_title('K-Medoids Davies-Bouldin Index (Lower is better)')\n",
        "axes[1,1].set_xlabel('k')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Algorithm Comparison\n",
        "Let's compare the performance of X-means and K-medoids algorithms by plotting their metrics side by side.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare X-means with K-medoids\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Find best k for k-medoids based on silhouette score\n",
        "best_k = k_range[np.argmax(kmedoids_metrics['silhouette'])]\n",
        "print(f\"Best k for K-medoids based on silhouette score: {best_k}\")\n",
        "\n",
        "# Plot comparison\n",
        "axes[0,0].plot([len(xmeans_centers)], [xmeans_sse], 'ro', markersize=10, label='X-means')\n",
        "axes[0,0].plot(k_range, kmedoids_metrics['sse'], 'bo-', label='K-medoids')\n",
        "axes[0,0].set_title('SSE Comparison')\n",
        "axes[0,0].set_xlabel('k')\n",
        "axes[0,0].legend()\n",
        "\n",
        "axes[0,1].plot([len(xmeans_centers)], [xmeans_silhouette], 'ro', markersize=10, label='X-means')\n",
        "axes[0,1].plot(k_range, kmedoids_metrics['silhouette'], 'bo-', label='K-medoids')\n",
        "axes[0,1].set_title('Silhouette Score Comparison')\n",
        "axes[0,1].set_xlabel('k')\n",
        "axes[0,1].legend()\n",
        "\n",
        "axes[1,0].plot([len(xmeans_centers)], [xmeans_vrc], 'ro', markersize=10, label='X-means')\n",
        "axes[1,0].plot(k_range, kmedoids_metrics['vrc'], 'bo-', label='K-medoids')\n",
        "axes[1,0].set_title('Variance Ratio Criterion Comparison')\n",
        "axes[1,0].set_xlabel('k')\n",
        "axes[1,0].legend()\n",
        "\n",
        "axes[1,1].plot([len(xmeans_centers)], [xmeans_davies_bouldin], 'ro', markersize=10, label='X-means')\n",
        "axes[1,1].plot(k_range, kmedoids_metrics['davies_bouldin'], 'bo-', label='K-medoids')\n",
        "axes[1,1].set_title('Davies-Bouldin Index Comparison')\n",
        "axes[1,1].set_xlabel('k')\n",
        "axes[1,1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nX-means Results:\")\n",
        "print(f\"  Number of clusters: {len(xmeans_centers)}\")\n",
        "print(f\"  SSE: {xmeans_sse:.2f}\")\n",
        "print(f\"  Silhouette: {xmeans_silhouette:.3f}\")\n",
        "print(f\"  VRC: {xmeans_vrc:.2f}\")\n",
        "print(f\"  Davies-Bouldin: {xmeans_davies_bouldin:.3f}\")\n",
        "\n",
        "print(f\"\\nK-medoids Results (k={best_k}):\")\n",
        "print(f\"  SSE: {kmedoids_metrics['sse'][best_k-2]:.2f}\")\n",
        "print(f\"  Silhouette: {kmedoids_metrics['silhouette'][best_k-2]:.3f}\")\n",
        "print(f\"  VRC: {kmedoids_metrics['vrc'][best_k-2]:.2f}\")\n",
        "print(f\"  Davies-Bouldin: {kmedoids_metrics['davies_bouldin'][best_k-2]:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation\n",
        "### Cluster Overview\n",
        "What do we do with the clusters now? What we can do is to look into each cluster and look into the average variables that it has. We create a table that has a row for each cluster and a column for each variable. The value in the cell is the rounded average of that variable for that cluster. In case of categorical variables, the most common category is listed with its percentage in braces. For boolean variables, both options are listed with their percentages. The columns (features) are sorted by their importance towards the clustering process, more on that in the next section.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cluster_overview(\n",
        "    original_df: pd.DataFrame,\n",
        "    cluster_labels: np.ndarray,\n",
        "    num_cols: List[str],\n",
        "    cat_cols: List[str],\n",
        "    bool_cols: List[str],\n",
        "    feature_importances: Dict[str, float],\n",
        ") -> pd.DataFrame:\n",
        "    def num_agg(series):\n",
        "        res = f\"~{series.mean():.2f}\"\n",
        "        if res == \"~-0.0\":\n",
        "            return \"~0.0\"\n",
        "        return res\n",
        "\n",
        "    def cat_agg(series):\n",
        "        vc = series.value_counts(normalize=True)\n",
        "        if len(vc) >= 1:\n",
        "            return f\"{vc.index[0]} ({vc.iloc[0]:.0%})\"\n",
        "        else:\n",
        "            return \"NA (100%)\"\n",
        "\n",
        "    def bool_agg(series):\n",
        "        vc = series.value_counts(normalize=True)\n",
        "        if len(vc) == 1:\n",
        "            return f\"{vc.iloc[0]:.0%} {vc.index[0]}\"\n",
        "        elif len(vc) == 2:\n",
        "            return f\"{vc.iloc[0]:.0%} {vc.index[0]}/{vc.iloc[1]:.0%} {vc.index[1]}\"\n",
        "        else:\n",
        "            return \"100% NA\"\n",
        "\n",
        "    df = original_df.copy(deep=True)\n",
        "    df[\"Cluster ID\"] = cluster_labels\n",
        "    \n",
        "    # Calculate per-cluster aggregations\n",
        "    df_clusters = pd.concat(\n",
        "        [\n",
        "            df[num_cols + [\"Cluster ID\"]].groupby(\"Cluster ID\").agg(num_agg) if num_cols else pd.DataFrame(),\n",
        "            df[cat_cols + [\"Cluster ID\"]].groupby(\"Cluster ID\").agg(cat_agg) if cat_cols else pd.DataFrame(),\n",
        "            df[bool_cols + [\"Cluster ID\"]].groupby(\"Cluster ID\").agg(bool_agg) if bool_cols else pd.DataFrame(),\n",
        "        ],\n",
        "        axis=1,\n",
        "    )\n",
        "    df_clusters = df_clusters.reset_index()\n",
        "    \n",
        "    # Calculate overall aggregations for entire dataset\n",
        "    df_overall = pd.concat(\n",
        "        [\n",
        "            df[num_cols].agg(num_agg).to_frame().T if num_cols else pd.DataFrame(),\n",
        "            df[cat_cols].agg(cat_agg).to_frame().T if cat_cols else pd.DataFrame(),\n",
        "            df[bool_cols].agg(bool_agg).to_frame().T if bool_cols else pd.DataFrame(),\n",
        "        ],\n",
        "        axis=1,\n",
        "    )\n",
        "    df_overall[\"Cluster ID\"] = \"All Clusters\"\n",
        "    \n",
        "    # Add member counts\n",
        "    counts = np.bincount(cluster_labels)\n",
        "    df_clusters[\"Members\"] = df_clusters.apply(lambda row: counts[int(row[\"Cluster ID\"])], axis=1)\n",
        "    df_overall[\"Members\"] = len(cluster_labels)\n",
        "    \n",
        "    # Format cluster IDs\n",
        "    df_clusters[\"Cluster ID\"] = df_clusters[\"Cluster ID\"].apply(lambda x: f\"Cluster {x}\")\n",
        "    \n",
        "    # Combine cluster data with overall data\n",
        "    df_result = pd.concat([df_clusters, df_overall], ignore_index=True)\n",
        "    \n",
        "    # Order features by importance\n",
        "    df_result = df_result[[\"Cluster ID\", \"Members\"] + list(feature_importances.keys())]\n",
        "\n",
        "    return df_result\n",
        "\n",
        "\n",
        "def feature_importances(\n",
        "    df: pd.DataFrame,\n",
        "    labels: np.ndarray\n",
        ") -> Dict[str, float]:\n",
        "    classifier = RandomForestClassifier(n_estimators=100)\n",
        "    classifier.fit(df, labels)\n",
        "    return dict(\n",
        "        sorted(\n",
        "            zip(df.columns, classifier.feature_importances_),\n",
        "            key=lambda it: it[1],\n",
        "            reverse=True,\n",
        "        )\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### X-Means Cluster Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# X-means cluster analysis\n",
        "xmeans_feature_importances = feature_importances(encoded_df, xmeans_labels)\n",
        "xmeans_cluster_overview = cluster_overview(og_df, xmeans_labels, num_cols, cat_cols, bool_cols, xmeans_feature_importances)\n",
        "print(\"X-Means Cluster Overview:\")\n",
        "xmeans_cluster_overview\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### K-Medoids Cluster Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# K-medoids cluster analysis (using best k)\n",
        "kmedoids_labels = kmedoids_results[best_k]\n",
        "kmedoids_feature_importances = feature_importances(encoded_df, kmedoids_labels)\n",
        "kmedoids_cluster_overview = cluster_overview(og_df, kmedoids_labels, num_cols, cat_cols, bool_cols, kmedoids_feature_importances)\n",
        "print(f\"K-Medoids Cluster Overview (k={best_k}):\")\n",
        "kmedoids_cluster_overview\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Importances\n",
        "The feature importance was already used above to sort the columns of the cluster overview table descending by their importance towards the clustering process. We can also plot it as a bar chart to get a better understanding of the features that contributed most to the separation of the clusters. \n",
        "\n",
        "We cannot directly extract the feature importance from the clustering models, but instead we can approximate the feature importances by training a supervised model on the cluster labels we generated. Some sort of decision tree models works well because of their explainability, which allows us to calculate the feature importances. We use a Random Forest Classifier here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
        "\n",
        "# X-means feature importance\n",
        "top_n = 20\n",
        "xmeans_top_features = list(xmeans_feature_importances.items())[:top_n]\n",
        "xmeans_features, xmeans_importances = zip(*xmeans_top_features)\n",
        "\n",
        "axes[0].barh(range(len(xmeans_features)), xmeans_importances, color='steelblue')\n",
        "axes[0].set_yticks(range(len(xmeans_features)))\n",
        "axes[0].set_yticklabels(xmeans_features)\n",
        "axes[0].set_xlabel('Importance')\n",
        "axes[0].set_ylabel('Feature')\n",
        "axes[0].set_title(f'Top {top_n} Feature Importances for X-Means Clustering')\n",
        "axes[0].invert_yaxis()\n",
        "\n",
        "# K-medoids feature importance\n",
        "kmedoids_top_features = list(kmedoids_feature_importances.items())[:top_n]\n",
        "kmedoids_features, kmedoids_importances = zip(*kmedoids_top_features)\n",
        "\n",
        "axes[1].barh(range(len(kmedoids_features)), kmedoids_importances, color='darkorange')\n",
        "axes[1].set_yticks(range(len(kmedoids_features)))\n",
        "axes[1].set_yticklabels(kmedoids_features)\n",
        "axes[1].set_xlabel('Importance')\n",
        "axes[1].set_ylabel('Feature')\n",
        "axes[1].set_title(f'Top {top_n} Feature Importances for K-Medoids Clustering (k={best_k})')\n",
        "axes[1].invert_yaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Artists per Cluster\n",
        "To validate whether the clustering makes sense, we can examine which artists belong to each cluster. This allows us to listen to the artists and verify if the clusters have meaningful musical or stylistic coherence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_artists_per_cluster(\n",
        "    original_df: pd.DataFrame,\n",
        "    cluster_labels: np.ndarray,\n",
        "    artist_column: str = 'name_artist',\n",
        "    top_n: int = 10\n",
        ") -> pd.DataFrame:\n",
        "    original_df['cluster'] = cluster_labels\n",
        "    \n",
        "    cluster_data = {}\n",
        "    for cluster_id in sorted(np.unique(cluster_labels)):\n",
        "        cluster_df = original_df[original_df['cluster'] == cluster_id]\n",
        "        \n",
        "        # Count tracks per artist in this cluster\n",
        "        artist_counts = cluster_df[artist_column].value_counts().head(top_n)\n",
        "        \n",
        "        # Format as \"Artist (N tracks)\"\n",
        "        formatted_artists = [f\"{artist} ({count})\" for artist, count in zip(artist_counts.index, artist_counts.values)]\n",
        "        \n",
        "        # Pad with empty strings if less than top_n artists\n",
        "        while len(formatted_artists) < top_n:\n",
        "            formatted_artists.append(\"\")\n",
        "        \n",
        "        cluster_data[f'Cluster {cluster_id}'] = formatted_artists\n",
        "    \n",
        "    df = pd.DataFrame(cluster_data)\n",
        "    return df\n",
        "\n",
        "\n",
        "# Load full dataset for artist analysis\n",
        "full_og_df = pd.read_csv('../dataset/tracks.csv')\n",
        "\n",
        "# X-means artists analysis\n",
        "print(\"X-Means Artists per Cluster:\")\n",
        "xmeans_artists_df = get_artists_per_cluster(full_og_df, xmeans_labels, top_n=10)\n",
        "xmeans_artists_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# K-medoids artists analysis\n",
        "print(f\"K-Medoids Artists per Cluster (k={best_k}):\")\n",
        "kmedoids_artists_df = get_artists_per_cluster(full_og_df, kmedoids_labels, top_n=10)\n",
        "kmedoids_artists_df\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
