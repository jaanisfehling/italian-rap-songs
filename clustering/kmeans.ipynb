{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd932b7d",
   "metadata": {},
   "source": [
    "# Subtask 1\n",
    "## K-means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f589e5",
   "metadata": {},
   "source": [
    "Requirements:\n",
    "- numpy\n",
    "- pandas\n",
    "- scikit-learn\n",
    "\n",
    "Python 3.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52e21bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fe9504",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Load the data and preprocess it. Since we are working with mixed-type data, we need to encode features that are not continuous numeric. Also, missing values need to be imputated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21f3148d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant preprocessing functions\n",
    "\n",
    "def clean(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Replace empty cells (or ?) with NA\n",
    "    df = df.replace(r\"^\\?|\\s+$\", pd.NA, regex=True)\n",
    "\n",
    "    # Columns with whitespace (or ?) values are infered as string type but could be numeric\n",
    "    for col in df.select_dtypes(exclude=[np.number, np.bool_]).columns:\n",
    "        # Exclude booleans\n",
    "        if df[col].nunique() > 2:\n",
    "            try:\n",
    "                df[col] = pd.to_numeric(df[col])\n",
    "            except ValueError:\n",
    "                logging.debug(\"Could not convert column %s to numeric dtype\", col)\n",
    "\n",
    "    # Integer columns with missing values automatically get converted to float columns\n",
    "    # We want to convert back to integer to allow for better column classification (num/cat)\n",
    "    for col in df.select_dtypes(include=np.floating):\n",
    "        if np.all(df[col].fillna(0) % 1 == 0):\n",
    "            df[col] = df[col].astype(\"Int64\")\n",
    "\n",
    "    # Turn object columns into string columns (causes issues with stepmix)\n",
    "    for col in df.select_dtypes(include=object).columns:\n",
    "        df[col] = df[col].astype(str)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def classify_cols(\n",
    "    df: pd.DataFrame,\n",
    "    bool_cols: bool = False,\n",
    ") -> Tuple:\n",
    "    # All floating point columns are considered numeric\n",
    "    num_cols: List[str] = list(df.select_dtypes(include=np.inexact).columns)\n",
    "    cat_cols: List[str] = []\n",
    "    bool_cols_: List[str] = list(df.select_dtypes(include=np.bool_).columns) if bool_cols else []\n",
    "\n",
    "    remaining_cols = df.columns.difference(num_cols + bool_cols_)\n",
    "\n",
    "    if bool_cols:\n",
    "        for col in remaining_cols:\n",
    "            if df[col].nunique() <= 2:\n",
    "                bool_cols_.append(col)\n",
    "                remaining_cols = remaining_cols.drop(col)\n",
    "\n",
    "    # Integer columns with more than 50 unique values are considered numeric\n",
    "    for col in df[remaining_cols].select_dtypes(include=np.integer).columns:\n",
    "        if df[col].nunique() >= 50:\n",
    "            num_cols.append(col)\n",
    "            remaining_cols = remaining_cols.drop(col)\n",
    "\n",
    "    # All other columns are considered categorical\n",
    "    cat_cols = list(remaining_cols)\n",
    "\n",
    "    logging.info(\"Num cols: %s\", num_cols)\n",
    "    logging.info(\"Cat cols: %s\", cat_cols)\n",
    "    logging.info(\"Bool cols: %s\", bool_cols_)\n",
    "    if bool_cols:\n",
    "        return num_cols, cat_cols, bool_cols_\n",
    "    return num_cols, cat_cols\n",
    "\n",
    "\n",
    "def scale_cols(\n",
    "    df: pd.DataFrame,\n",
    "    num_cols: Optional[List[str]],\n",
    "    cat_cols: Optional[List[str]],\n",
    "    bool_cols: Optional[List[str]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    df = df.copy(deep=True)\n",
    "    if num_cols:\n",
    "        df[num_cols] = StandardScaler().fit_transform(df[num_cols])\n",
    "    if cat_cols:\n",
    "        df[cat_cols] = df[cat_cols].apply(LabelEncoder().fit_transform)\n",
    "    if bool_cols:\n",
    "        df[bool_cols] = df[bool_cols].apply(LabelEncoder().fit_transform)\n",
    "    return df\n",
    "\n",
    "\n",
    "def imputate_na(\n",
    "    df: pd.DataFrame,\n",
    "    num_cols: Optional[List[str]],\n",
    "    cat_cols: Optional[List[str]],\n",
    "    bool_cols: Optional[List[str]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    if num_cols:\n",
    "        # imputate numeric features\n",
    "        for col in num_cols:\n",
    "            df[col] = df[col].fillna(round(df[col].median()))\n",
    "    if cat_cols is None:\n",
    "        cat_cols = []\n",
    "    if bool_cols is None:\n",
    "        bool_cols = []\n",
    "    # add \"missing\" category for categorical features\n",
    "    for col in cat_cols + bool_cols:\n",
    "        df[col] = df[col].astype(str)\n",
    "        df[col] = df[col].fillna(\"<NA>\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced13fb5",
   "metadata": {},
   "source": [
    "### Cleaning, Imputating, Scaling and Labelling\n",
    "Now execute the functions from above to return a dataframe we can use for computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7ab0536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id  id_artist  name_artist  full_title  title  featured_artists  \\\n",
      "0  10202          5           82       11135  10484               485   \n",
      "1   8089          5           82        7444   6986              1579   \n",
      "2   9973          5           82       11106  10455              1004   \n",
      "3   4665          5           82        1415   1306              1555   \n",
      "4   5981          5           82        5064   4646              1382   \n",
      "\n",
      "   primary_artist  language  album  stats_pageviews  ...  album_type  \\\n",
      "0              82        24    240         4.382687  ...           0   \n",
      "1              82         9    240         3.566031  ...           0   \n",
      "2              82         9    240         0.821598  ...           0   \n",
      "3              82        17    555         0.345227  ...           3   \n",
      "4              82         9    240         0.229954  ...           0   \n",
      "\n",
      "   disc_number  track_number  duration_ms  explicit  popularity  album_image  \\\n",
      "0            0            22     0.048662         1          82         1056   \n",
      "1            0            22     0.048662         1          82         1056   \n",
      "2            0             0    -0.112995         1          72         1056   \n",
      "3            0            11    -0.392077         1          83          997   \n",
      "4            0            37    -0.098952         1          75         2456   \n",
      "\n",
      "   id_album  lyrics  modified_popularity  \n",
      "0        47    7180                    0  \n",
      "1        47    4077                    0  \n",
      "2        47    6183                    0  \n",
      "3      2123    2690                    0  \n",
      "4      1123    1670                    0  \n",
      "\n",
      "[5 rows x 45 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../dataset/tracks.csv\")\n",
    "\n",
    "df = clean(df)\n",
    "num_cols, cat_cols, bool_cols = classify_cols(df, bool_cols=True)\n",
    "df = imputate_na(df, num_cols, cat_cols, bool_cols)\n",
    "df = scale_cols(df, num_cols, cat_cols, bool_cols)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570753f0",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "Now we use one-hot encoding to encode the categorical features. Boolean features only need encoding if they contain NaN or NA values (=missing values), otherwise they are left as is. To reduce the number of features, we only encode features that have more unique values than a threshhold. This will get rid of features with too many unique values, such as song lyrics. These features would add up to N new features to the dataset, which is not a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4daf6978",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Column album has too many unique values, dropping\n",
      "WARNING:root:Column album_image has too many unique values, dropping\n",
      "WARNING:root:Column album_name has too many unique values, dropping\n",
      "WARNING:root:Column album_release_date has too many unique values, dropping\n",
      "WARNING:root:Column day has too many unique values, dropping\n",
      "WARNING:root:Column featured_artists has too many unique values, dropping\n",
      "WARNING:root:Column full_title has too many unique values, dropping\n",
      "WARNING:root:Column id has too many unique values, dropping\n",
      "WARNING:root:Column id_album has too many unique values, dropping\n",
      "WARNING:root:Column id_artist has too many unique values, dropping\n",
      "WARNING:root:Column language has too many unique values, dropping\n",
      "WARNING:root:Column lyrics has too many unique values, dropping\n",
      "WARNING:root:Column name_artist has too many unique values, dropping\n",
      "WARNING:root:Column popularity has too many unique values, dropping\n",
      "WARNING:root:Column primary_artist has too many unique values, dropping\n",
      "WARNING:root:Column swear_EN has too many unique values, dropping\n",
      "WARNING:root:Column swear_EN_words has too many unique values, dropping\n",
      "WARNING:root:Column swear_IT has too many unique values, dropping\n",
      "WARNING:root:Column swear_IT_words has too many unique values, dropping\n",
      "WARNING:root:Column title has too many unique values, dropping\n",
      "WARNING:root:Column track_number has too many unique values, dropping\n",
      "WARNING:root:Column year has too many unique values, dropping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   stats_pageviews  n_sentences  n_tokens  tokens_per_sent  char_per_tok  \\\n",
      "0         4.382687     1.730216  1.986653         0.046638      0.261922   \n",
      "1         3.566031    -0.137659  0.854572         0.598627      0.510317   \n",
      "2         0.821598     1.161733  1.252719        -0.009536      0.047712   \n",
      "3         0.345227    -0.909173 -0.550936         0.292905     -0.067872   \n",
      "4         0.229954    -0.462507 -0.325479         0.047721     -0.295177   \n",
      "\n",
      "   lexical_density  avg_token_per_clause       bpm  centroid   rolloff  ...  \\\n",
      "0         0.786870              0.009550  0.793368  1.642220  2.259997  ...   \n",
      "1         1.792820              0.310083  0.570975  2.354421  3.112774  ...   \n",
      "2         0.535180              0.029394  0.732070  1.558628  0.743616  ...   \n",
      "3         0.227492             -0.089032  1.798812 -1.066151 -0.813071  ...   \n",
      "4        -0.358439              0.028674 -0.307387  0.181037  0.135803  ...   \n",
      "\n",
      "   month_3  month_4  month_5  month_6  month_7  month_8  month_9  month_10  \\\n",
      "0        0        0        0        1        0        0        0         0   \n",
      "1        0        0        1        0        0        0        0         0   \n",
      "2        0        1        0        0        0        0        0         0   \n",
      "3        0        0        1        0        0        0        0         0   \n",
      "4        0        0        0        0        1        0        0         0   \n",
      "\n",
      "   month_11  month_12  \n",
      "0         0         0  \n",
      "1         0         0  \n",
      "2         0         0  \n",
      "3         0         0  \n",
      "4         0         0  \n",
      "\n",
      "[5 rows x 45 columns]\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode_feature(df: pd.DataFrame, feature_to_encode: str, weight: float = 1) -> pd.DataFrame:\n",
    "    dummies = pd.get_dummies(df[feature_to_encode], dtype=\"int32\", prefix=feature_to_encode)\n",
    "    dummies *= weight\n",
    "    result_df = pd.concat([df, dummies], axis=1)\n",
    "    return result_df.drop(columns=feature_to_encode)\n",
    "\n",
    "\n",
    "encoded_df = df.copy(deep=True)\n",
    "\n",
    "for col in cat_cols + [col for col in bool_cols if encoded_df[col].nunique() > 2]:\n",
    "    # Dont encode columns with too many unique values\n",
    "    if encoded_df[col].nunique() > 25:\n",
    "        logging.warning(\"Column %s has too many unique values, dropping\", col)\n",
    "        encoded_df = encoded_df.drop(columns=col)\n",
    "    else:\n",
    "        encoded_df = one_hot_encode_feature(encoded_df, col)\n",
    "\n",
    "print(encoded_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db12dff",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "We can now cluster the songs using K-means clustering. We use k-means++ to get a better initialization. The algorithm is run 100 times and inertia is used to determine the best result. Inertia is the sum of squared distances between each data point and its assigned centroid. Lower inertia means better clustering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ece4ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 2], shape=(11166,), dtype=int32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = KMeans(n_clusters=3, init=\"k-means++\", n_init=100).fit_predict(encoded_df)\n",
    "labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851c5de4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
